{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_name = \"../dataset/user_twitter_data.csv\"\n",
    "\n",
    "# Lire le fichier CSV\n",
    "data = pd.read_csv(file_path_name)\n",
    "\n",
    "# Extraire les colonnes des caractéristiques\n",
    "features = data.columns[1:]  # Supposant que 'user_id' est la première colonne\n",
    "\n",
    "# Extraire les données des caractéristiques\n",
    "X = data[features]\n",
    "\n",
    "# Calculer et sauvegarder la moyenne et le coefficient de variation de toutes les caractéristiques\n",
    "mean_var_all = pd.DataFrame({\n",
    "    'Mean': X.mean(),\n",
    "    'CV': X.std() / (X.mean() +1e-10)\n",
    "})\n",
    "mean_var_all.to_csv('all_mean_cv_wo_standard.csv', index_label='Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardiser les données\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k(X_scaled, split_num, K_max=10):\n",
    "    silhouette_scores = []\n",
    "    k_range = range(2, K_max + 1)\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(X_scaled)\n",
    "        labels = kmeans.labels_\n",
    "        silhouette_scores.append(silhouette_score(X_scaled, labels))\n",
    "\n",
    "    # Tracer le coefficient de silhouette\n",
    "    plt.plot(k_range, silhouette_scores)\n",
    "    plt.xlabel('Nombre de clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Score for K-means Clustering')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Sauvegarder le graphique dans un fichier\n",
    "    plt.savefig('Silhouette_Score_K-means_Clustering' + '_cluste_' +str(split_num + 2) + '.png')\n",
    "    plt.clf()\n",
    "    best_k = k_range[np.argmax(silhouette_scores)]\n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données en 9 parties\n",
    "num_splits = 9\n",
    "split_data = np.array_split(data, num_splits)\n",
    "\n",
    "# Initialiser une liste vide pour stocker le nombre optimal de clusters pour chaque partie\n",
    "best_k_list = []\n",
    "\n",
    "# Traiter chaque partie des données\n",
    "for i, split in enumerate(split_data):\n",
    "    X_split = split[features]\n",
    "    X_split_scaled = scaler.transform(X_split)\n",
    "    best_k = find_best_k(X_split_scaled, i)\n",
    "    best_k_list.append(best_k)\n",
    "    print(f\"Best number of clusters for split {i+1}: {best_k}\")\n",
    "\n",
    "# Compter le nombre d'apparitions de chaque nombre optimal de clusters\n",
    "best_k_counts = Counter(best_k_list)\n",
    "print(\"Best K counts:\", best_k_counts)\n",
    "\n",
    "# Choisir le nombre optimal de clusters le plus fréquent\n",
    "final_best_k = best_k_counts.most_common(1)[0][0]\n",
    "print(f\"Final best number of clusters by majority vote: {final_best_k}\")\n",
    "\n",
    "# Stocker la moyenne et le CV de chaque cluster\n",
    "cluster_stats = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir une fonction pour effectuer le clustering K-means et trouver les outliers\n",
    "def find_outliers(X_scaled, k):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(X_scaled)\n",
    "    distances = kmeans.transform(X_scaled)\n",
    "    min_distances = np.min(distances, axis=1)\n",
    "    threshold = np.mean(min_distances) + 2 * np.std(min_distances)\n",
    "    outliers = min_distances > threshold\n",
    "    return outliers\n",
    "# Initialiser un DataFrame vide pour stocker tous les outliers\n",
    "# all_outliers_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Effectuer le clustering K-means sur l'ensemble du dataset standardisé\n",
    "kmeans = KMeans(n_clusters=final_best_k, random_state=0).fit(X_scaled)\n",
    "labels = kmeans.labels_\n",
    "data['Cluster'] = labels  # Assigner les labels des clusters aux données\n",
    "\n",
    "# Initialiser un DataFrame vide pour collecter les statistiques des clusters\n",
    "cluster_stats = pd.DataFrame()\n",
    "\n",
    "# Calculer les statistiques des clusters pour chaque cluster\n",
    "for cluster in range(final_best_k):\n",
    "    cluster_data = data[data['Cluster'] == cluster][features]\n",
    "    cluster_summary = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        f'Cluster {cluster+1} Mean': cluster_data.mean(),\n",
    "        f'Cluster {cluster+1} CV': cluster_data.std() / (cluster_data.mean() +1e-10)\n",
    "    })\n",
    "    if cluster_stats.empty:\n",
    "        cluster_stats = cluster_summary\n",
    "    else:\n",
    "        cluster_stats = pd.concat([cluster_stats, cluster_summary], axis=1)\n",
    "\n",
    "# Identifier et rassembler les outliers si nécessaire\n",
    "outliers = find_outliers(X_scaled, final_best_k)\n",
    "outliers_data = data[outliers]\n",
    "all_outliers_data = outliers_data  # Supposant que vous voulez stocker toutes les données des outliers\n",
    "\n",
    "# Afficher toutes les informations des outliers\n",
    "print(\"Final Outliers:\")\n",
    "print(all_outliers_data)\n",
    "\n",
    "all_outliers_data.to_csv('all_outliers_data.csv', index=False, sep=',', lineterminator='\\n')\n",
    "print(\"Les données des outliers ont été sauvegardées dans all_outliers_data.csv\")\n",
    "\n",
    "# Afficher uniquement les IDs de tous les outliers\n",
    "all_outliers_id = all_outliers_data[\"user_id\"].values\n",
    "print(\"Final Outliers IDs:\")\n",
    "print(all_outliers_id)\n",
    "np.savetxt('all_outliers_id.csv', all_outliers_id, delimiter=',', header='Index,Value', comments='', fmt='%d')\n",
    "print(\"Les IDs des outliers ont été sauvegardés dans all_outliers_id.csv\")\n",
    "\n",
    "cluster_stats.to_csv('clusters_mean_cv_wo_standard.csv', index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total running time: {:.2f} seconds\".format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
